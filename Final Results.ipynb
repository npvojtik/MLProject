{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import supporting packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load training and test data\n",
    "filename1 = 'training.csv'\n",
    "accounts = pd.read_csv(filename1)\n",
    "filename2 = 'test.csv'\n",
    "test = pd.read_csv(filename2)\n",
    "test = test[:575] # error in reading loads 1000 rows; there's only data for 575 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean NaNs out of the data\n",
    "def remove_NaN(text):\n",
    "    if (pd.isnull(text)):\n",
    "        text = \"\"\n",
    "    else:\n",
    "        text = text\n",
    "    return text\n",
    "\n",
    "accounts['description'] = accounts['description'].apply(remove_NaN)\n",
    "accounts['location'] = accounts['location'].apply(remove_NaN)\n",
    "accounts['url'] = accounts['description'].apply(remove_NaN)\n",
    "accounts['status'] = accounts['status'].apply(remove_NaN)\n",
    "accounts['has_extended_profile'] = accounts['has_extended_profile'].apply(remove_NaN)\n",
    "\n",
    "test['description'] = test['description'].apply(remove_NaN)\n",
    "test['location'] = test['location'].apply(remove_NaN)\n",
    "test['url'] = test['description'].apply(remove_NaN)\n",
    "test['status'] = test['status'].apply(remove_NaN)\n",
    "test['has_extended_profile'] = test['has_extended_profile'].apply(remove_NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a column that indicates if the word \"bot\" appears in the screen name\n",
    "def bot_in_name(text):\n",
    "    if \"bot\" in text.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def bot_in_desc(text):\n",
    "    text = text.lower()\n",
    "    if \"bot,\" in text:\n",
    "        return 1\n",
    "    if \" bot \" in text:\n",
    "        return 1\n",
    "    if \" bot.\" in text:\n",
    "        return 1\n",
    "    if \"#bot\" in text:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "accounts['bot_in_screen_name'] = accounts.screen_name.apply(bot_in_name)\n",
    "test['bot_in_screen_name'] = test.screen_name.apply(bot_in_name)\n",
    "accounts['bot_in_desc'] = accounts.description.apply(bot_in_desc)\n",
    "test['bot_in_desc'] = test.description.apply(bot_in_desc)\n",
    "accounts['truther_in_screen_name'] = accounts.screen_name.apply(truther_in_name)\n",
    "test['truther_in_screen_name'] = test.screen_name.apply(truther_in_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean the test data (there's some issues with 0 counts for followers/friends)\n",
    "# (and in some places verified=0 is coded as None)\n",
    "\n",
    "def clean_verified(text):\n",
    "    if text.lower() == 'true':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def clean_counts(text):\n",
    "    if text == \"None\":\n",
    "        return 0\n",
    "    else:\n",
    "        return int(text)\n",
    "    \n",
    "test.verified = test.verified.apply(clean_verified)\n",
    "test.friends_count = test.friends_count.apply(clean_counts)\n",
    "test.followers_count = test.followers_count.apply(clean_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the text of the last status at the time of data collection\n",
    "def return_text(text):\n",
    "    try:\n",
    "        if (\"'text':\" in text):\n",
    "            start = text.index(\"'text': '\") + 9\n",
    "            end = text.index(\"',\", start)\n",
    "            return text[start:end]\n",
    "        if ('\"text\":' in text):\n",
    "            start = text.index('\"text\": \"') + 9\n",
    "            end = text.index('\",', start)\n",
    "            return text[start:end]\n",
    "        if (text != \"null\" and text != \"None\"):\n",
    "            return text\n",
    "        if not pd.isnull(text):\n",
    "            return \"\"\n",
    "    except:\n",
    "        start = text.index(\"'text'\") + 10\n",
    "        end = text.index(\",\", start + 1) - 1\n",
    "        return text[start:end]\n",
    "    \n",
    "accounts['text'] = accounts.status.apply(return_text)\n",
    "test['text'] = test.status.apply(return_text)\n",
    "\n",
    "def bot_in_text(text):\n",
    "    if (\"bot\") in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "accounts['bot_in_text'] = accounts.text.apply(bot_in_text)\n",
    "test['bot_in_text'] = test.text.apply(bot_in_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add column for lexical diversity of the description\n",
    "# lexical diversity function (as written in Shellman [2])\n",
    "# http://www.erinshellman.com/bot-or-not/\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    if (pd.isnull(text)):\n",
    "        diversity = 0\n",
    "    elif len(text) == 0:\n",
    "        diversity = 0\n",
    "    else:\n",
    "        diversity = float(len(set(text))) / len(text)\n",
    "    return diversity\n",
    "\n",
    "accounts['desc_lex_diversity'] = accounts['description'].apply(lexical_diversity)\n",
    "accounts['text_lex_diversity'] = accounts['text'].apply(lexical_diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bots:\n",
      "====================================================================================================\n",
      "Followers:\n",
      "Avg: 13458.7737\t\tStd: 215307.6876\tMed: 99.0000\n",
      "\n",
      "Friends:\n",
      "Avg: 1611.3747\t\tStd: 17841.9368\t\tMed: 14.0000\n",
      "\n",
      "Description Lexical Diversity:\n",
      "Avg: 0.3141\t\tStd: 0.1988\t\tMed: 0.2936\n",
      "\n",
      "Text Lexical Diversity:\n",
      "Avg: 0.3960\t\tStd: 0.2537\t\tMed: 0.3704\n",
      "\n",
      "Statuses:\n",
      "Avg: 29249.1703\t\tStd: 222066.0451\t\tMed: 1560.0000\n",
      "\n",
      "Humans:\n",
      "====================================================================================================\n",
      "Followers:\n",
      "Avg: 1876165.4546\tStd: 7591517.0999\tMed: 4422.5000\n",
      "\n",
      "Friends:\n",
      "Avg: 7647.1308\t\tStd: 75387.9973\t\tMed: 382.5000\n",
      "\n",
      "Description Lexical Diversity:\n",
      "Avg: 0.3445\t\tStd: 0.2196\t\tMed: 0.3193\n",
      "\n",
      "Text Lexical Diversity:\n",
      "Avg: 0.3817\t\tStd: 0.2069\t\tMed: 0.3496\n",
      "\n",
      "Statuses:\n",
      "Avg: 10683.1023\t\tStd: 29660.2321\t\tMed: 2931.5000\n"
     ]
    }
   ],
   "source": [
    "# split the accounts into bots and non-bots, then look at basic statistics\n",
    "# for friend count, follower count, and lexical diversity of the description and the text\n",
    "bots = accounts.loc[accounts['bot'] == 1]\n",
    "humans = accounts.loc[accounts['bot'] == 0]\n",
    "\n",
    "print(\"Bots:\")\n",
    "print(\"=\"*100)\n",
    "print(\"Followers:\")\n",
    "print(\"Avg: {:.4f}\\t\\tStd: {:.4f}\\tMed: {:.4f}\".format(bots.followers_count.mean(), bots.followers_count.std(), bots.followers_count.median()))\n",
    "print(\"\\nFriends:\")\n",
    "print(\"Avg: {:.4f}\\t\\tStd: {:.4f}\\t\\tMed: {:.4f}\".format(bots.friends_count.mean(), bots.friends_count.std(), bots.friends_count.median()))\n",
    "print(\"\\nDescription Lexical Diversity:\")\n",
    "print(\"Avg: {:.4f}\\t\\tStd: {:.4f}\\t\\tMed: {:.4f}\".format(bots.desc_lex_diversity.mean(), bots.desc_lex_diversity.std(), bots.desc_lex_diversity.median()))\n",
    "print(\"\\nText Lexical Diversity:\")\n",
    "print(\"Avg: {:.4f}\\t\\tStd: {:.4f}\\t\\tMed: {:.4f}\".format(bots.text_lex_diversity.mean(), bots.text_lex_diversity.std(), bots.text_lex_diversity.median()))\n",
    "print(\"\\nStatuses:\")\n",
    "print(\"Avg: {:.4f}\\t\\tStd: {:.4f}\\t\\tMed: {:.4f}\".format(bots.statuses_count.mean(), bots.statuses_count.std(), bots.statuses_count.median()))\n",
    "print(\"\\nHumans:\")\n",
    "print(\"=\"*100)\n",
    "print(\"Followers:\")\n",
    "print(\"Avg: {:.4f}\\tStd: {:.4f}\\tMed: {:.4f}\".format(humans.followers_count.mean(), humans.followers_count.std(), humans.followers_count.median()))\n",
    "print(\"\\nFriends:\")\n",
    "print(\"Avg: {:.4f}\\t\\tStd: {:.4f}\\t\\tMed: {:.4f}\".format(humans.friends_count.mean(), humans.friends_count.std(), humans.friends_count.median()))\n",
    "print(\"\\nDescription Lexical Diversity:\")\n",
    "print(\"Avg: {:.4f}\\t\\tStd: {:.4f}\\t\\tMed: {:.4f}\".format(humans.desc_lex_diversity.mean(), humans.desc_lex_diversity.std(), humans.desc_lex_diversity.median()))\n",
    "print(\"\\nText Lexical Diversity:\")\n",
    "print(\"Avg: {:.4f}\\t\\tStd: {:.4f}\\t\\tMed: {:.4f}\".format(humans.text_lex_diversity.mean(), humans.text_lex_diversity.std(), humans.text_lex_diversity.median()))\n",
    "print(\"\\nStatuses:\")\n",
    "print(\"Avg: {:.4f}\\t\\tStd: {:.4f}\\t\\tMed: {:.4f}\".format(humans.statuses_count.mean(), humans.statuses_count.std(), humans.statuses_count.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification ratio for bots: 0.0061\n",
      "Verification ratio for humans: 0.4883\n"
     ]
    }
   ],
   "source": [
    "# get the ratio of verified accounts\n",
    "b_verified = bots['verified'].sum() / len(bots.index)\n",
    "h_verified = humans['verified'].sum() / len(bots.index)\n",
    "\n",
    "print(\"Verification ratio for bots: {:.4f}\".format(b_verified))\n",
    "print(\"Verification ratio for humans: {:.4f}\".format(h_verified))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.83768953344876729, 0.83874462408370776, 0.84479702687249847)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = 5\n",
    "\n",
    "X = accounts[['bot_in_screen_name', 'bot_in_text', 'verified', 'friends_count', 'followers_count', 'statuses_count', 'bot_in_desc']]\n",
    "y = accounts.bot\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X, y)\n",
    "\n",
    "dt_accuracy = sum(cross_val_score(dt, X, y, cv=cv, scoring='accuracy'))/cv\n",
    "dt_precision = sum(cross_val_score(dt, X, y, cv=cv, scoring='precision'))/cv\n",
    "dt_recall = sum(cross_val_score(dt, X, y, cv=cv, scoring='recall'))/cv\n",
    "dt_accuracy, dt_precision, dt_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.67569603219398022, 0.59647166652864192, 0.9735020011435106)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X, y)\n",
    "\n",
    "mnb_accuracy = sum(cross_val_score(mnb, X, y, cv=cv, scoring='accuracy'))/cv\n",
    "mnb_precision = sum(cross_val_score(mnb, X, y, cv=cv, scoring='precision'))/cv\n",
    "mnb_recall = sum(cross_val_score(mnb, X, y, cv=cv, scoring='recall'))/cv\n",
    "mnb_accuracy, mnb_precision, mnb_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.76332450039700384, 0.94480609869692, 0.52991995425957683)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb = BernoulliNB(alpha=1)\n",
    "bnb.fit(X, y)\n",
    "\n",
    "bnb_accuracy = sum(cross_val_score(bnb, X, y, cv=cv, scoring='accuracy'))/cv\n",
    "bnb_precision = sum(cross_val_score(bnb, X, y, cv=cv, scoring='precision'))/cv\n",
    "bnb_recall = sum(cross_val_score(bnb, X, y, cv=cv, scoring='recall'))/cv\n",
    "bnb_accuracy, bnb_precision, bnb_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6871386707228021, 0.60795932487185289, 0.95079473985134366)"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=.001, penalty=\"l1\")\n",
    "lr.fit(X, y)\n",
    "\n",
    "lr_accuracy = sum(cross_val_score(lr, X, y, cv=cv, scoring='accuracy'))/cv\n",
    "lr_precision = sum(cross_val_score(lr, X, y, cv=cv, scoring='precision'))/cv\n",
    "lr_recall = sum(cross_val_score(lr, X, y, cv=cv, scoring='recall'))/cv\n",
    "lr_accuracy, lr_precision, lr_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.87771644679989413, 0.92093232446366324, 0.80996569468267587)"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_jobs = 5, n_estimators = 1000, max_features = 4, max_depth = 5)\n",
    "rf.fit(X, y)\n",
    "\n",
    "rf_accuracy = sum(cross_val_score(rf, X, y, cv=cv, scoring='accuracy'))/cv\n",
    "rf_precision = sum(cross_val_score(rf, X, y, cv=cv, scoring='precision'))/cv\n",
    "rf_recall = sum(cross_val_score(rf, X, y, cv=cv, scoring='recall'))/cv\n",
    "rf_accuracy, rf_precision, rf_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.82444012895449281, 0.79114604590425919, 0.85311034877072611)"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10\n",
    "# train the k-nearest neighbors model with k = 10\n",
    "for weights in ['uniform', 'distance']:\n",
    "    knn = KNeighborsClassifier(k, weights=weights)\n",
    "    knn.fit(X, y)\n",
    "    \n",
    "knn_accuracy = sum(cross_val_score(knn, X, y, cv=cv, scoring='accuracy'))/cv\n",
    "knn_precision = sum(cross_val_score(knn, X, y, cv=cv, scoring='precision'))/cv\n",
    "knn_recall = sum(cross_val_score(knn, X, y, cv=cv, scoring='recall'))/cv\n",
    "knn_accuracy, knn_precision, knn_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.87879234308782872, 0.92041615806196919, 0.81072041166380782)"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc = VotingClassifier(estimators=[('dt', dt), ('mnb', mnb), ('bnb', bnb), ('lr', lr), ('rf', rf), ('knn', knn)], voting='hard', weights=[5,1,1,1,8,1])\n",
    "vc.fit(X, y)\n",
    "\n",
    "vc_accuracy = sum(cross_val_score(vc, X, y, cv=cv, scoring='accuracy'))/cv\n",
    "vc_precision = sum(cross_val_score(vc, X, y, cv=cv, scoring='precision'))/cv\n",
    "vc_recall = sum(cross_val_score(vc, X, y, cv=cv, scoring='recall'))/cv\n",
    "vc_accuracy, vc_precision, vc_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1408, 68, 215, 1106)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = vc.predict(X)\n",
    "tn, fp, fn, tp = confusion_matrix(y, predictions).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1389, 87, 214, 1107)"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(predictions)):\n",
    "    if (X.bot_in_screen_name[i] == 1):\n",
    "        predictions[i] = 1\n",
    "    if (X.bot_in_desc[i] == 1):\n",
    "        predictions[i] = 1\n",
    "    if (X.bot_in_text[i] == 1):\n",
    "        predictions[i] = 1\n",
    "        \n",
    "tn, fp, fn, tp = confusion_matrix(y, predictions).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now process the test data\n",
    "data = test[['bot_in_screen_name', 'bot_in_text', 'verified', 'friends_count', 'followers_count', 'statuses_count', 'bot_in_desc']]\n",
    "predictions = vc.predict(data)\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    if (data.bot_in_screen_name[i] == 1):\n",
    "        predictions[i] = 1\n",
    "    if (data.bot_in_desc[i] == 1):\n",
    "        predictions[i] = 1\n",
    "    if (data.bot_in_text[i] == 1):\n",
    "        predictions[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bots = predictions.tolist()\n",
    "ids = test.id.tolist()\n",
    "\n",
    "with open('output.txt', 'w') as file:\n",
    "    file.write(\"Id,Bot\\n\")\n",
    "    for i in range(len(bots)):\n",
    "        file.write(\"{:.0f},{:d}\\n\".format(ids[i], bots[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.74938759371043906, 0.77720485744836443, 0.65784734133790734)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions = accounts['description'].tolist()\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X = count_vect.fit_transform(descriptions)\n",
    "y = accounts.bot\n",
    "\n",
    "mn = MultinomialNB()\n",
    "mn.fit(X, y)\n",
    "mnb_accuracy = sum(cross_val_score(mnb, X, y, cv=cv, scoring='accuracy'))/cv\n",
    "mnb_precision = sum(cross_val_score(mnb, X, y, cv=cv, scoring='precision'))/cv\n",
    "mnb_recall = sum(cross_val_score(mnb, X, y, cv=cv, scoring='recall'))/cv\n",
    "mnb_accuracy, mnb_precision, mnb_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.75367587269092051, 0.83257776707149167, 0.59880503144654085)"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = BernoulliNB()\n",
    "bn.fit(X, y)\n",
    "bnb_accuracy = sum(cross_val_score(bnb, X, y, cv=cv, scoring='accuracy'))/cv\n",
    "bnb_precision = sum(cross_val_score(bnb, X, y, cv=cv, scoring='precision'))/cv\n",
    "bnb_recall = sum(cross_val_score(bnb, X, y, cv=cv, scoring='recall'))/cv\n",
    "bnb_accuracy, bnb_precision, bnb_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.74507890650161512, 0.81798185723997352, 0.59197541452258429)"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X, y)\n",
    "lr_accuracy = sum(cross_val_score(lr, X, y, cv=cv, scoring='accuracy'))/cv\n",
    "lr_precision = sum(cross_val_score(lr, X, y, cv=cv, scoring='precision'))/cv\n",
    "lr_recall = sum(cross_val_score(lr, X, y, cv=cv, scoring='recall'))/cv\n",
    "lr_accuracy, lr_precision, lr_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.60170153603806131, 0.6032948436172374, 0.52992853058890799)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for weights in ['uniform', 'distance']:\n",
    "    knn = KNeighborsClassifier(k, weights=weights)\n",
    "    knn.fit(X, y)\n",
    "\n",
    "knn_accuracy = sum(cross_val_score(knn, X, y, cv=cv, scoring='accuracy'))/cv\n",
    "knn_precision = sum(cross_val_score(knn, X, y, cv=cv, scoring='precision'))/cv\n",
    "knn_recall = sum(cross_val_score(knn, X, y, cv=cv, scoring='recall'))/cv\n",
    "knn_accuracy, knn_precision, knn_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.63030494357443756, 0.69400758800758788, 0.38984276729559747)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = accounts['text'].tolist()\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X = count_vect.fit_transform(text)\n",
    "y = accounts.bot\n",
    "\n",
    "mn = MultinomialNB()\n",
    "mn.fit(X, y)\n",
    "mnb_accuracy = sum(cross_val_score(mnb, X, y, cv=cv, scoring='accuracy'))/cv\n",
    "mnb_precision = sum(cross_val_score(mnb, X, y, cv=cv, scoring='precision'))/cv\n",
    "mnb_recall = sum(cross_val_score(mnb, X, y, cv=cv, scoring='recall'))/cv\n",
    "mnb_accuracy, mnb_precision, mnb_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.67641287121451277, 0.63590902405900096, 0.73806174957118353)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = BernoulliNB()\n",
    "bn.fit(X, y)\n",
    "bnb_accuracy = sum(cross_val_score(bnb, X, y, cv=cv, scoring='accuracy'))/cv\n",
    "bnb_precision = sum(cross_val_score(bnb, X, y, cv=cv, scoring='precision'))/cv\n",
    "bnb_recall = sum(cross_val_score(bnb, X, y, cv=cv, scoring='recall'))/cv\n",
    "bnb_accuracy, bnb_precision, bnb_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6639147446260989, 0.62391081160652195, 0.72594911377930238)"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X, y)\n",
    "lr_accuracy = sum(cross_val_score(lr, X, y, cv=cv, scoring='accuracy'))/cv\n",
    "lr_precision = sum(cross_val_score(lr, X, y, cv=cv, scoring='precision'))/cv\n",
    "lr_recall = sum(cross_val_score(lr, X, y, cv=cv, scoring='recall'))/cv\n",
    "lr_accuracy, lr_precision, lr_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.47871389896013694, 0.46593535509449974, 0.78421097770154369)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for weights in ['uniform', 'distance']:\n",
    "    knn = KNeighborsClassifier(k, weights=weights)\n",
    "    knn.fit(X, y)\n",
    "\n",
    "knn_accuracy = sum(cross_val_score(knn, X, y, cv=cv, scoring='accuracy'))/cv\n",
    "knn_precision = sum(cross_val_score(knn, X, y, cv=cv, scoring='precision'))/cv\n",
    "knn_recall = sum(cross_val_score(knn, X, y, cv=cv, scoring='recall'))/cv\n",
    "knn_accuracy, knn_precision, knn_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
